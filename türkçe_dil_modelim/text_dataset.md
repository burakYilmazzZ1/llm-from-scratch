
# ğŸ“„ TextDataset â€“ Dil Modeli iÃ§in EÄŸitim Verisi HazÄ±rlama

Bu proje, **tokenize edilmiÅŸ bir metni**, bir dil modelini (GPT tarzÄ± *next-token prediction*) eÄŸitmek iÃ§in uygun hale getiren Ã¶zel bir `PyTorch Dataset` sÄ±nÄ±fÄ± iÃ§erir.

AmaÃ§:

> Uzun bir token dizisini, **sabit uzunlukta baÄŸlam pencerelerine (context window)** bÃ¶lmek ve her giriÅŸ iÃ§in **bir sonraki tokenâ€™larÄ± hedef (target)** olarak Ã¼retmek.

---

## ğŸ¯ Genel MantÄ±k

Dil modelleri ÅŸu problemi Ã¶ÄŸrenir:

> **â€œBu token dizisini gÃ¶rdÃ¼ÄŸÃ¼mde bir sonraki token ne olmalÄ±?â€**

Bunu yapmak iÃ§in:

* Girdi (`input`):

  ```
  [t0, t1, t2, ..., t(n-1)]
  ```
* Hedef (`target`):

  ```
  [t1, t2, t3, ..., t(n)]
  ```

Yani hedef dizisi, girdinin **1 token kaydÄ±rÄ±lmÄ±ÅŸ hali**dir.

---

## ğŸ§± Kodun YapÄ±sÄ±

### 1ï¸âƒ£ `TextDataset` SÄ±nÄ±fÄ±

```python
class TextDataset(Dataset):
```

Bu sÄ±nÄ±f:

* `torch.utils.data.Dataset`â€™ten tÃ¼retilmiÅŸtir
* PyTorch `DataLoader` ile uyumlu Ã§alÄ±ÅŸÄ±r
* Token dizisini parÃ§alara ayÄ±rÄ±r ve modelin anlayacaÄŸÄ± tensÃ¶rlere Ã§evirir

---

### 2ï¸âƒ£ Girdi Parametreleri

```python
def __init__(self, token_ids, context_length, stride):
```

| Parametre        | AÃ§Ä±klama                                         |
| ---------------- | ------------------------------------------------ |
| `token_ids`      | Tokenizerâ€™dan Ã§Ä±kmÄ±ÅŸ **uzun bir token listesi**  |
| `context_length` | Modelin aynÄ± anda gÃ¶receÄŸi maksimum token sayÄ±sÄ± |
| `stride`         | Sliding windowâ€™un ne kadar kayarak ilerleyeceÄŸi  |

ğŸ“Œ **stride kÃ¼Ã§Ã¼kse:** daha fazla Ã¶rnek, daha Ã§ok veri
ğŸ“Œ **stride bÃ¼yÃ¼kse:** daha az Ã¶rnek, daha hÄ±zlÄ± eÄŸitim

---

### 3ï¸âƒ£ Sliding Window (KaydÄ±rmalÄ± Pencere) MantÄ±ÄŸÄ±

```python
for i in range(0, len(token_ids)-context_length, stride):
```

Bu dÃ¶ngÃ¼:

* Token dizisi Ã¼zerinde ilerler
* Her adÄ±mda `context_length` uzunluÄŸunda bir pencere alÄ±r
* Pencereyi `stride` kadar kaydÄ±rÄ±r

Ã–rnek:

```text
Token dizisi: [1,2,3,4,5,6,7,8]
context_length = 4
stride = 2
```

OluÅŸan giriÅŸler:

```
[1,2,3,4]
[3,4,5,6]
[5,6,7,8]
```

---

### 4ï¸âƒ£ Input ve Target OluÅŸturma

```python
input_chunk  = token_ids[i : i + context_length]
target_chunk = token_ids[i+1 : i + context_length + 1]
```

Burada:

* `input_chunk` â†’ modelin gÃ¶rdÃ¼ÄŸÃ¼ veri
* `target_chunk` â†’ modelin tahmin etmeye Ã§alÄ±ÅŸtÄ±ÄŸÄ± veri

ğŸ“Œ **1 token kaydÄ±rma**, dil modelinin temel Ã¶ÄŸrenme prensibidir.

---

### 5ï¸âƒ£ Padding (Eksik Token Tamamlama)

```python
input_chunk  = input_chunk  + [pad_id] * (context_length - len(input_chunk))
target_chunk = target_chunk + [pad_id] * (context_length - len(target_chunk))
```

AmaÃ§:

* TÃ¼m Ã¶rneklerin **aynÄ± uzunlukta** olmasÄ±
* Batch iÅŸlemlerinin sorunsuz Ã§alÄ±ÅŸmasÄ±

ğŸ“Œ `pad_id`:

* Tokenizerâ€™da Ã¶zel olarak tanÄ±mlanmÄ±ÅŸ **PAD token IDâ€™si**
* Genellikle loss hesaplanÄ±rken maskelenir

---

### 6ï¸âƒ£ Tensorâ€™a DÃ¶nÃ¼ÅŸtÃ¼rme

```python
torch.tensor(input_chunk, dtype=torch.long)
```

Neden?

* PyTorch modelleri **tensor** ile Ã§alÄ±ÅŸÄ±r
* `long` â†’ embedding katmanlarÄ± iÃ§in zorunludur

---

### 7ï¸âƒ£ Dataset FonksiyonlarÄ±

```python
def __len__(self):
```

* Datasetâ€™te kaÃ§ Ã¶rnek olduÄŸunu dÃ¶ndÃ¼rÃ¼r

```python
def __getitem__(self, idx):
```

* Belirli bir index iÃ§in `(input, target)` dÃ¶ndÃ¼rÃ¼r

Bu iki fonksiyon sayesinde `DataLoader` Ã§alÄ±ÅŸabilir.

---

### 8ï¸âƒ£ DataLoader OluÅŸturma

```python
def create_data_loader(token_ids, context_length, stride, batch_size, shuffle=True):
```

Bu fonksiyon:

* `TextDataset` oluÅŸturur
* PyTorch `DataLoader` dÃ¶ndÃ¼rÃ¼r

AvantajlarÄ±:

* Mini-batch eÄŸitim
* Shuffle desteÄŸi
* GPU uyumluluÄŸu

---

## ğŸ” EÄŸitimde NasÄ±l KullanÄ±lÄ±r?

```python
loader = TextDataset.create_data_loader(
    token_ids=token_ids,
    context_length=128,
    stride=64,
    batch_size=32
)

for inputs, targets in loader:
    logits = model(inputs)
    loss = loss_fn(logits.view(-1, vocab_size), targets.view(-1))
```

