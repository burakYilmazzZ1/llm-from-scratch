# TÃ¼rkÃ§e Morfem TabanlÄ± Tokenizer

Bu repository, **TÃ¼rkÃ§e iÃ§in Ã¶zel olarak tasarlanmÄ±ÅŸ, morfoloji farkÄ±ndalÄ±ÄŸÄ± olan bir tokenizer** iÃ§erir. Proje, eklemeli (agglutinative) bir dil olan TÃ¼rkÃ§eâ€™nin yapÄ±sal Ã¶zelliklerini doÄŸrudan tokenizasyon aÅŸamasÄ±na entegre etmeyi amaÃ§lar.

Tokenizer, Ã¶zel olarak geliÅŸtirilmiÅŸ bir **Transformer mimarisi** ile birlikte Ã§alÄ±ÅŸacak ÅŸekilde tasarlanmÄ±ÅŸtÄ±r.

---

## ğŸ“Œ Temel Motivasyon

Klasik tokenizer yaklaÅŸÄ±mlarÄ± (BPE, WordPiece):

* TÃ¼rkÃ§e ekleri rastgele bÃ¶ler
* Ã‡ok bÃ¼yÃ¼k vocabulary Ã¼retir
* Dilbilgisel bilgiyi modele bÄ±rakÄ±r

Ã–rnek:

```
kitaplarÄ±mdan
â†’ kitaplar ##Ä±m ##dan
```

Bu proje ile hedeflenen:

```
kitap + lar + Ä±m + dan
```

ÅŸeklinde **dilbilgisel olarak anlamlÄ±** bir tokenizasyon elde etmektir.

---

## ğŸ§  Tokenizer TasarÄ±m Prensipleri

* Morfem temelli parÃ§alama
* Unigram Language Model ile istatistiksel seÃ§im
* Eklerin fonksiyonel bilgisini modele aktarma
* EÄŸitim ve inference aÅŸamalarÄ±nÄ±n ayrÄ±lmasÄ±

---

## ğŸ§© Tokenizer Mimarisi

### Genel AkÄ±ÅŸ

```
Raw Text
   â†“
Normalization
   â†“
Word Segmentation (space token yok)
   â†“
Zemberek Morfem Analizi
   â†“
Unigram LM Token SeÃ§imi
   â†“
Token ID + Token Type ID
```

---

## 1ï¸âƒ£ Unigram Language Model (Greedy Yerine)

### AmaÃ§

Greedy algoritmalar yerine, **en olasÄ± morfem dizisini** seÃ§mek.

### Neden?

Greedy yÃ¶ntemler yalnÄ±zca lokal en uzun eÅŸleÅŸmeye bakar. Unigram LM ise tÃ¼m olasÄ± tokenizasyonlarÄ± deÄŸerlendirir.

### Ã–rnek Kod

```python
# Unigram LM skor hesaplama (basitleÅŸtirilmiÅŸ)
def score(tokens, token_probs):
    return sum(token_probs.get(t, -1e9) for t in tokens)
```

---

## 2ï¸âƒ£ Zemberek ile Otomatik Morfem BÃ¶lme

### AmaÃ§

Tokenizerâ€™Ä±n ekleri tahmin etmesi yerine **bilerek ayÄ±rmasÄ±**.

### KullanÄ±m

```python
from zemberek import TurkishMorphology

morphology = TurkishMorphology.create_with_defaults()
analysis = morphology.analyze("kitaplarÄ±mdan")

for result in analysis:
    print(result.get_stem(), result.get_suffixes())
```

### KazanÄ±m

* Dilbilgisel doÄŸruluk
* Daha az Ã¶ÄŸrenme yÃ¼kÃ¼

---

## 3ï¸âƒ£ Space Token Yerine Pozisyonel AyrÄ±m

### AmaÃ§

BoÅŸluÄŸu vocabularyâ€™den Ã§Ä±karmak.

### YaklaÅŸÄ±m

* Space ayrÄ± token deÄŸildir
* Kelime sÄ±nÄ±rlarÄ± pozisyonel embedding ile modellenir

```python
# space token eklenmez
vocab = {"[PAD]": 0, "[UNK]": 1}
```

---

## 4ï¸âƒ£ Eklerin Token Type ID ile Encode Edilmesi

### AmaÃ§

Tokenâ€™Ä±n **ne olduÄŸu** bilgisini modele ayrÄ± bir kanal olarak vermek.

### Token Type ÅemasÄ±

```python
TOKEN_TYPES = {
    "ROOT": 0,
    "PLURAL_SUFFIX": 1,
    "CASE_SUFFIX": 2,
    "POSSESSIVE_SUFFIX": 3,
    "VERB_TENSE": 4
}
```

### Ã–rnek Encoding

```python
tokens = ["kitap", "lar", "Ä±m", "dan"]
token_ids = [1021, 204, 317, 411]
token_type_ids = [0, 1, 3, 2]
```

---

## 5ï¸âƒ£ Auto-Learn MekanizmasÄ± (Sadece Train-Time)

### AmaÃ§

Inference sÄ±rasÄ±nda tokenizer davranÄ±ÅŸÄ±nÄ±n deÄŸiÅŸmesini Ã¶nlemek.

### MantÄ±k

```python
class Tokenizer:
    def __init__(self, train_mode=False):
        self.train_mode = train_mode

    def add_token(self, token):
        if self.train_mode:
            self.vocab[token] = len(self.vocab)
```

---

## ğŸ”’ Deterministik Inference

* Vocabulary inference sÄ±rasÄ±nda sabittir
* Embedding uyumsuzluÄŸu oluÅŸmaz
* SonuÃ§lar reproducibleâ€™dÄ±r

---

## ğŸ“Š Klasik Tokenizer KarÅŸÄ±laÅŸtÄ±rmasÄ±

| Ã–zellik             | BPE / WordPiece | Bu Tokenizer |
| ------------------- | --------------- | ------------ |
| TÃ¼rkÃ§e uyumu        | DÃ¼ÅŸÃ¼k           | YÃ¼ksek       |
| Morfem farkÄ±ndalÄ±ÄŸÄ± | Yok             | Var          |
| Vocabulary boyutu   | BÃ¼yÃ¼k           | Daha kÃ¼Ã§Ã¼k   |
| Dilbilgisel bilgi   | Ã–ÄŸrenilmeli     | Entegre      |

---

## ğŸš€ Hedeflenen KullanÄ±m AlanlarÄ±

* TÃ¼rkÃ§e LLMâ€™ler
* Akademik NLP araÅŸtÄ±rmalarÄ±
* Dilbilgisel farkÄ±ndalÄ±k gerektiren gÃ¶revler

---

## ğŸ“Œ Not

Bu tokenizer, TÃ¼rkÃ§e iÃ§in **inductive bias** eklemeyi amaÃ§layan deneysel bir Ã§alÄ±ÅŸmadÄ±r ve klasik tokenizerâ€™larÄ±n birebir alternatifi deÄŸil, **dil-Ã¶zel bir Ã§Ã¶zÃ¼m** olarak tasarlanmÄ±ÅŸtÄ±r.
