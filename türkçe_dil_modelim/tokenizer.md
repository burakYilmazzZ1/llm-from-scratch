---

# TÃ¼rkÃ§e Morfem-TabanlÄ± Tokenizer

Bu repository, **TÃ¼rkÃ§e iÃ§in Ã¶zel olarak tasarlanmÄ±ÅŸ**,
**JSON tabanlÄ±**, **subword (alt-birim) odaklÄ±** ve **dinamik Ã¶ÄŸrenme destekli** bir tokenizer implementasyonu iÃ§erir.

Tokenizer, Ã¶zellikle **TÃ¼rkÃ§enin eklemeli (agglutinative) yapÄ±sÄ±nÄ±** dikkate alarak geliÅŸtirilmiÅŸtir ve bir Transformer / LLM pipelineâ€™Ä±nda doÄŸrudan kullanÄ±labilir.

---

## ğŸ“Œ Temel Ã–zellikler

* ğŸ“ **JSON tabanlÄ± vocab**
* ğŸ§© **Subword (alt-parÃ§a) tokenization**
* ğŸ” **Greedy (en uzun eÅŸleÅŸme) algoritmasÄ±**
* ğŸ§  **Dinamik vocab geniÅŸletme (auto-learn)**
* ğŸ·ï¸ **Kategorilere ayrÄ±lmÄ±ÅŸ token yapÄ±sÄ±**
* ğŸ”¤ **BÃ¼yÃ¼k harf bilgisi iÃ§in Ã¶zel token**
* ğŸ§ª **Batch encoding + padding desteÄŸi**
* ğŸ”„ **Encode / Decode fonksiyonlarÄ±**

---

## ğŸ“‚ Vocab YapÄ±sÄ±

Tokenizer, vocabâ€™Ä± **kategorilere ayrÄ±lmÄ±ÅŸ bir JSON dosyasÄ±ndan** yÃ¼kler.

Ã–rnek yapÄ±:

```json
{
  "Ã¶zel_tokenler": {
    "<pad>": 0,
    "<unk>": 1,
    "<baÅŸla>": 2,
    "<bitiÅŸ>": 3,
    "<bÃ¼yÃ¼k_harf>": 4
  },
  "kelimeler": {
    "ankara": 10,
    "tÃ¼rkiye": 11
  },
  "ekler": {
    "ler": 100,
    "lar": 101,
    "de": 102,
    "da": 103
  },
  "karakterler": {
    "a": 300,
    "b": 301
  }
}
```

Tokenizer bu yapÄ±yÄ±:

* Tek bir `vocab` sÃ¶zlÃ¼ÄŸÃ¼nde birleÅŸtirir
* AynÄ± zamanda **kategori bilgisini** korur

---

## âš™ï¸ SÄ±nÄ±f: `Tokenizer`

### BaÅŸlatma

```python
tokenizer = Tokenizer(
    vocab_file="vocab.json",
    auto_learn=True
)
```

**Parametreler:**

* `vocab_file`: Tokenlerin bulunduÄŸu JSON dosyasÄ±
* `auto_learn`: Vocabâ€™da olmayan karakterleri otomatik ekler

---

## ğŸ”¢ Encode (Metin â†’ Token ID)

```python
ids = tokenizer.encode(
    "Ankara TÃ¼rkiye'dedir",
    add_uppercase_token=True,
    add_special_tokens=True
)
```

### Encode sÄ±rasÄ±nda yapÄ±lan iÅŸlemler:

1. Metin temizlenir (`strip`)
2. Ä°steÄŸe baÄŸlÄ± olarak:

   * `<baÅŸla>` tokeni eklenir
   * BÃ¼yÃ¼k harfle baÅŸlÄ±yorsa `<bÃ¼yÃ¼k_harf>` eklenir
3. Metin kelimelere ayrÄ±lÄ±r
4. Her kelime **greedy subword tokenization** ile parÃ§alanÄ±r
5. Kelimeler arasÄ±na **space token** eklenir
6. En sona isteÄŸe baÄŸlÄ± `<bitiÅŸ>` tokeni eklenir

---

## ğŸ§© Subword Tokenization (Greedy)

Tokenizer, her kelimeyi **en uzun parÃ§adan baÅŸlayarak** vocabâ€™da arar:

Ã–rnek:

```
"kitaplardan"
â†’ kitap + lar + dan
```

Algoritma:

* En uzun eÅŸleÅŸme aranÄ±r
* Bulunamazsa karakter seviyesine dÃ¼ÅŸÃ¼lÃ¼r
* Karakter de yoksa:

  * `auto_learn=True` ise vocabâ€™a eklenir
  * Aksi halde `<unk>` kullanÄ±lÄ±r

---

## ğŸ“¦ Batch Encode + Padding

```python
batch = tokenizer.encode_batch(
    texts=["Merhaba dÃ¼nya", "Ankara"],
    context_length=16
)
```

* Her cÃ¼mle encode edilir
* Uzunsa **truncate**
* KÄ±saysa `<pad>` ile doldurulur
* Ã‡Ä±ktÄ±: `(batch_size, context_length)` tensor

---

## ğŸ” Decode (Token ID â†’ Metin)

```python
text = tokenizer.decode(ids)
```

* Token IDâ€™ler tekrar stringâ€™e Ã§evrilir
* VarsayÄ±lan olarak:

  * `<pad>`, `<baÅŸla>`, `<bitiÅŸ>`, `<unk>`, `<bÃ¼yÃ¼k_harf>` **Ã§Ä±karÄ±lÄ±r**

---

## ğŸ§ª Debug AmaÃ§lÄ± Token GÃ¶sterimi

```python
tokenizer.tokenize("Ankara")
```

Ã‡Ä±ktÄ±:

```python
["<bÃ¼yÃ¼k_harf>", "ankara"]
```

---

## â• Dinamik Token Ekleme

### Tek token ekleme

```python
tokenizer.add_token("den", category="ekler")
```

### Ã‡oklu token ekleme

```python
tokenizer.add_tokens(["miÅŸ", "mÄ±ÅŸ"], category="ekler")
```

* Token IDâ€™ler otomatik atanÄ±r
* Hem vocabâ€™a hem kategoriye eklenir

---

## ğŸ’¾ Vocab Kaydetme

```python
tokenizer.save_vocab("yeni_vocab.json")
```

* GÃ¼ncel vocab
* Kategoriler korunarak JSONâ€™a yazÄ±lÄ±r

---

## ğŸ“Š Ã–ÄŸrenme Ä°statistikleri

```python
stats = tokenizer.get_learning_stats()
```

Ã–rnek Ã§Ä±ktÄ±:

```json
{
  "toplam_token_sayÄ±sÄ±": 742,
  "kategori_sayÄ±sÄ±": 6,
  "kategoriler": ["kelimeler", "ekler", "karakterler"],
  "sonraki_token_id": 743,
  "auto_learn_aktif": true
}
```

---

## ğŸ¯ TasarÄ±m AmacÄ±

Bu tokenizer:

* TÃ¼rkÃ§enin **eklemeli yapÄ±sÄ±na uygun**
* KÃ¼Ã§Ã¼k vocab ile **yÃ¼ksek kapsama**
* EÄŸitim sÄ±rasÄ±nda **kendini geniÅŸletebilen**
* Transformer tabanlÄ± modellerle **doÄŸrudan uyumlu**

bir yapÄ± sunmayÄ± hedefler.

---

## ğŸ“Œ Not

Bu implementasyon:

* Greedy subword yaklaÅŸÄ±mÄ± kullanÄ±r
* Space token aÃ§Ä±kÃ§a temsil edilir
* Morfolojik analiz **harici** deÄŸil, vocab Ã¼zerinden yapÄ±lÄ±r

---
