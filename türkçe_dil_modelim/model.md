# Rotary Positional Embedding (RoPE) TabanlÄ± Embedding ModÃ¼lÃ¼

Bu proje, **PyTorch** kullanÄ±larak sÄ±fÄ±rdan yazÄ±lmÄ±ÅŸ, **Rotary Positional Encoding (RoPE)** mantÄ±ÄŸÄ±nÄ± temel alan bir embedding katmanÄ±nÄ± iÃ§erir. AmaÃ§; klasik pozisyon embeddingâ€™lerine (absolute positional embedding) alternatif olarak, **token vektÃ¶rlerinin kendi iÃ§inde dÃ¶ndÃ¼rÃ¼lmesi (rotation)** yoluyla pozisyon bilgisini modele kazandÄ±rmaktÄ±r.

Bu yapÄ± Ã¶zellikle **Transformer tabanlÄ± dil modellerinde** (LLM) tercih edilen, modern ve matematiksel olarak gÃ¼Ã§lÃ¼ bir yaklaÅŸÄ±mdÄ±r.

---

## ğŸ“Œ Genel BakÄ±ÅŸ

Kod iki ana parÃ§adan oluÅŸur:

1. **`get_rotary_position_encoding` fonksiyonu**
   GÃ¶mÃ¼lÃ¼ (embedded) token vektÃ¶rlerine RoPE uygular.

2. **`Embedding` sÄ±nÄ±fÄ± (`nn.Module`)**
   Token embedding + rotary positional encoding iÅŸlemini tek bir modÃ¼lde birleÅŸtirir.

---

## ğŸ”¢ Rotary Positional Encoding (RoPE) Nedir?

RoPE, pozisyon bilgisini vektÃ¶re **eklemek yerine**, vektÃ¶rÃ¼ pozisyona baÄŸlÄ± olarak **sinÃ¼sâ€“kosinÃ¼s dÃ¶nÃ¼ÅŸÃ¼mÃ¼yle dÃ¶ndÃ¼rÃ¼r**.

Bu yaklaÅŸÄ±mÄ±n avantajlarÄ±:

* Relative (gÃ¶reli) pozisyon bilgisini doÄŸal olarak kodlar
* Uzun baÄŸlamlarda (long context) daha kararlÄ±dÄ±r
* Attention mekanizmasÄ±yla matematiksel olarak uyumludur

FormÃ¼l olarak her boyut Ã§ifti iÃ§in:

* `x_even' = x_even Â· cos(Î¸) âˆ’ x_odd Â· sin(Î¸)`
* `x_odd'  = x_even Â· sin(Î¸) + x_odd Â· cos(Î¸)`

Buradaki `Î¸`, token pozisyonuna ve boyuta baÄŸlÄ±dÄ±r.

---

## âš™ï¸ Fonksiyon: `get_rotary_position_encoding`

### Girdi

* **`input`** â†’ `[batch_size, context_length, embedding_dim]`
* **`base`** â†’ Frekans Ã¶lÃ§eÄŸi (varsayÄ±lan: `10000`)
* **`device`** â†’ CPU / GPU seÃ§imi

### AdÄ±m AdÄ±m Ne YapÄ±lÄ±yor?

1. **Boyut kontrolÃ¼**
   Embedding boyutunun Ã§ift olmasÄ± zorunludur.

2. **Boyutu ikiye bÃ¶lme**
   Embedding vektÃ¶rÃ¼ `even` ve `odd` parÃ§alarÄ±na ayrÄ±lÄ±r.

3. **FrekanslarÄ±n hesaplanmasÄ±**
   Her boyut iÃ§in farklÄ± aÃ§Ä±sal frekans Ã¼retilir.

4. **Pozisyon aÃ§Ä±larÄ±nÄ±n oluÅŸturulmasÄ±**
   Her token pozisyonu iÃ§in sinÃ¼s ve kosinÃ¼s deÄŸerleri hesaplanÄ±r.

5. **Rotasyon iÅŸlemi**
   VektÃ¶r Ã§iftleri sinâ€“cos dÃ¶nÃ¼ÅŸÃ¼mÃ¼yle dÃ¶ndÃ¼rÃ¼lÃ¼r.

6. **BirleÅŸtirme**
   DÃ¶ndÃ¼rÃ¼lmÃ¼ÅŸ parÃ§alar tekrar tek embedding haline getirilir.

### Ã‡Ä±ktÄ±

* Pozisyon bilgisi **gÃ¶mÃ¼lÃ¼**, aynÄ± boyutta tensor

---

## ğŸ§  SÄ±nÄ±f: `Embedding`

Bu sÄ±nÄ±f, PyTorchâ€™un `nn.Module` yapÄ±sÄ±nÄ± kullanarak **Ã¶ÄŸrenilebilir token embedding** ile **RoPE**â€™u birleÅŸtirir.

### `__init__`

* `vocab_size` â†’ Kelime daÄŸarcÄ±ÄŸÄ± boyutu
* `embedding_dim` â†’ Embedding vektÃ¶r boyutu
* `context_length` â†’ Maksimum baÄŸlam uzunluÄŸu
* `device` â†’ Ã‡alÄ±ÅŸma cihazÄ±

Ä°Ã§eride:

* `nn.Embedding` tanÄ±mlanÄ±r
* Rotary positional encoding fonksiyonu baÄŸlanÄ±r

### `forward(x)`

1. Token IDâ€™leri embedding vektÃ¶rlerine Ã§evrilir
2. Rotary positional encoding uygulanÄ±r
3. Son embedding dÃ¶ndÃ¼rÃ¼lÃ¼r

---

## ğŸ“ Tensor AkÄ±ÅŸÄ±

```
Input Token IDs
      â†“
nn.Embedding
      â†“
[token embedding]
      â†“
Rotary Positional Encoding
      â†“
[final embedding with position info]
```

---

## âœ… Neden Bu YaklaÅŸÄ±m?

* Absolute positional embedding yok â†’ daha esnek
* Relative pozisyon iliÅŸkileri korunur
* GPTâ€‘NeoX, LLaMA gibi modern modellerle uyumlu
* Kendi LLMâ€™ini yazmak isteyenler iÃ§in temiz ve Ã¶ÄŸretici bir Ã¶rnek

---

## ğŸš€ KullanÄ±m SenaryolarÄ±

* Kendi Transformer / LLM modelini yazanlar
* Positional encoding mantÄ±ÄŸÄ±nÄ± derinlemesine Ã¶ÄŸrenmek isteyenler
* GPTâ€‘2 tabanlÄ± ama daha modern bir positional yapÄ± denemek isteyenler

---

## ğŸ“Œ Notlar

* Embedding boyutu **mutlaka Ã§ift** olmalÄ±dÄ±r
* RoPE genellikle **Q ve K matrislerine** uygulanÄ±r; burada Ã¶ÄŸretici olmasÄ± iÃ§in embedding seviyesinde uygulanmÄ±ÅŸtÄ±r

---

## âœ¨ SonuÃ§

Bu modÃ¼l, sÄ±fÄ±rdan bir dil modeli geliÅŸtirme sÃ¼recinde **modern positional encoding yaklaÅŸÄ±mlarÄ±nÄ±** anlamak ve uygulamak iÃ§in saÄŸlam bir temel sunar.

